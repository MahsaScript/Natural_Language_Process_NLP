{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_IDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "zSsE736C3Bsy",
        "outputId": "fe519f95-f5d4-49ab-d6ef-3305b8ed249f"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Jun  6 19:47:44 2021\n",
        "\n",
        "@author: Mahsa\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "import numpy as np\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "data = \"Mars is approximately half the diameter of Earth.\"\n",
        "# print(word_tokenize(data))\n",
        "data = \"Mars is a cold desert world. It is half the size of Earth. \"\n",
        "# print(sent_tokenize(data))\n",
        "file_docs = []\n",
        "with open ('demofile.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs.append(line)\n",
        "# print(\"Number of documents:\",len(file_docs))\n",
        "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs]\n",
        "# print(gen_docs)\n",
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "# print(dictionary.token2id)\n",
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "# print(corpus)\n",
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "\n",
        "for doc in tf_idf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
        "sims = gensim.similarities.Similarity('newfolder',tf_idf[corpus],\n",
        "                                        num_features=len(dictionary))\n",
        "print(sims)\n",
        "file2_docs = []\n",
        "with open ('demofile2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "# print(document_number, document_similarity)\n",
        "print('Comparing Result:', sims[query_doc_tf_idf]) \n",
        "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)\n",
        "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')\n",
        "avg_sims = [] # array of averages\n",
        "\n",
        "# for line in query documents\n",
        "for line in file2_docs:\n",
        "        # tokenize words\n",
        "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "        # create bag of words\n",
        "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "        # find similarity for each document\n",
        "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "        # print (document_number, document_similarity)\n",
        "        print('Comparing Result:', sims[query_doc_tf_idf]) \n",
        "        # calculate sum of similarities for each query doc\n",
        "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "        # calculate average of similarity for each query doc\n",
        "        avg = sum_of_sims / len(file_docs)\n",
        "        # print average of similarity for each query doc\n",
        "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
        "        # add average values into array\n",
        "        avg_sims.append(avg)  \n",
        "    # calculate total average\n",
        "total_avg = np.sum(avg_sims, dtype=np.float)\n",
        "    # round the value and multiply by 100 to format it as percentage\n",
        "percentage_of_similarity = round(float(total_avg) * 100)\n",
        "    # if percentage is greater than 100\n",
        "    # that means documents are almost same\n",
        "if percentage_of_similarity >= 100:\n",
        "    percentage_of_similarity = 100\n",
        "    \n",
        "with open('demofile.txt') as documentA:\n",
        "    bagOfWordsA=[word for line in documentA for word in line.split()]\n",
        "\n",
        "with open('demofile2.txt') as documentB:\n",
        "    bagOfWordsB=[word for line in documentB for word in line.split()]\n",
        "\n",
        "\n",
        "\n",
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
        "\n",
        "numOfWordsA = dict.fromkeys(uniqueWords,0)\n",
        "\n",
        "for word in bagOfWordsA: \n",
        "    numOfWordsA[word] +=1\n",
        "\n",
        "numOfWordsB = dict.fromkeys(uniqueWords,0)\n",
        "\n",
        "for word in bagOfWordsB: \n",
        "    numOfWordsB[word] +=1\n",
        "    \n",
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    \n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] =count/ float(bagOfWordsCount)\n",
        "    return tfDict\n",
        "\n",
        "tfA= computeTF(numOfWordsA, bagOfWordsA)\n",
        "\n",
        "tfB= computeTF(numOfWordsB, bagOfWordsB)\n",
        "\n",
        "def cosinesim(vec1, vec2):\n",
        "    commonterms = set(vec1).intersection(vec2)\n",
        "    sim = 0.0\n",
        "    for token in commonterms:\n",
        "        sim += vec1[token]*vec2[token]\n",
        "\n",
        "    return sim\n",
        "\n",
        "print('Similarity TF_A and TF_B:',cosinesim(tfA, tfB))\n",
        "\n",
        "\n",
        "\n",
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    \n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val >0:\n",
        "                idfDict[word] += 1\n",
        "                \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N/float(val)) \n",
        "    \n",
        "    return idfDict\n",
        "               \n",
        "   \n",
        "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
        "print('Similarity idfs A and B:',idfs)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04a7875c44c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfile_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'demofile.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mfile_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    }
  ]
}