{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_IDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSsE736C3Bsy",
        "outputId": "bd004092-1f60-4cf1-d0e1-6020b00ef4ac"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Jun  6 19:47:44 2021\n",
        "\n",
        "@author: Mahsa\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "import numpy as np\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "data = \"Mars is approximately half the diameter of Earth.\"\n",
        "# print(word_tokenize(data))\n",
        "data = \"Mars is a cold desert world. It is half the size of Earth. \"\n",
        "# print(sent_tokenize(data))\n",
        "file_docs = []\n",
        "nltk.download('punkt')\n",
        "\n",
        "with open ('demofile.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs.append(line)\n",
        "# print(\"Number of documents:\",len(file_docs))\n",
        "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs]\n",
        "# print(gen_docs)\n",
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "# print(dictionary.token2id)\n",
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "# print(corpus)\n",
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "\n",
        "for doc in tf_idf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
        "sims = gensim.similarities.Similarity('newfolder',tf_idf[corpus],\n",
        "                                        num_features=len(dictionary))\n",
        "print(sims)\n",
        "file2_docs = []\n",
        "with open ('demofile2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "# print(document_number, document_similarity)\n",
        "print('Comparing Result:', sims[query_doc_tf_idf]) \n",
        "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)\n",
        "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')\n",
        "avg_sims = [] # array of averages\n",
        "\n",
        "# for line in query documents\n",
        "for line in file2_docs:\n",
        "        # tokenize words\n",
        "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "        # create bag of words\n",
        "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "        # find similarity for each document\n",
        "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "        # print (document_number, document_similarity)\n",
        "        print('Comparing Result:', sims[query_doc_tf_idf]) \n",
        "        # calculate sum of similarities for each query doc\n",
        "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "        # calculate average of similarity for each query doc\n",
        "        avg = sum_of_sims / len(file_docs)\n",
        "        # print average of similarity for each query doc\n",
        "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
        "        # add average values into array\n",
        "        avg_sims.append(avg)  \n",
        "    # calculate total average\n",
        "total_avg = np.sum(avg_sims, dtype=np.float)\n",
        "    # round the value and multiply by 100 to format it as percentage\n",
        "percentage_of_similarity = round(float(total_avg) * 100)\n",
        "    # if percentage is greater than 100\n",
        "    # that means documents are almost same\n",
        "if percentage_of_similarity >= 100:\n",
        "    percentage_of_similarity = 100\n",
        "    \n",
        "with open('demofile.txt') as documentA:\n",
        "    bagOfWordsA=[word for line in documentA for word in line.split()]\n",
        "\n",
        "with open('demofile2.txt') as documentB:\n",
        "    bagOfWordsB=[word for line in documentB for word in line.split()]\n",
        "\n",
        "\n",
        "\n",
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
        "\n",
        "numOfWordsA = dict.fromkeys(uniqueWords,0)\n",
        "\n",
        "for word in bagOfWordsA: \n",
        "    numOfWordsA[word] +=1\n",
        "\n",
        "numOfWordsB = dict.fromkeys(uniqueWords,0)\n",
        "\n",
        "for word in bagOfWordsB: \n",
        "    numOfWordsB[word] +=1\n",
        "    \n",
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    \n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] =count/ float(bagOfWordsCount)\n",
        "    return tfDict\n",
        "\n",
        "tfA= computeTF(numOfWordsA, bagOfWordsA)\n",
        "\n",
        "tfB= computeTF(numOfWordsB, bagOfWordsB)\n",
        "\n",
        "def cosinesim(vec1, vec2):\n",
        "    commonterms = set(vec1).intersection(vec2)\n",
        "    sim = 0.0\n",
        "    for token in commonterms:\n",
        "        sim += vec1[token]*vec2[token]\n",
        "\n",
        "    return sim\n",
        "\n",
        "print('Similarity TF_A and TF_B:',cosinesim(tfA, tfB))\n",
        "\n",
        "\n",
        "\n",
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    \n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val >0:\n",
        "                idfDict[word] += 1\n",
        "                \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N/float(val)) \n",
        "    \n",
        "    return idfDict\n",
        "               \n",
        "   \n",
        "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
        "print('Similarity idfs A and B:',idfs)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[['.', 0.45], ['are', 0.45], ['good', 0.45], ['malls', 0.45], ['shopping', 0.45]]\n",
            "[['?', 0.14], ['bread', 0.37], ['is', 0.37], ['kind', 0.37], ['of', 0.37], ['sandwiches', 0.37], ['used', 0.37], ['what', 0.37]]\n",
            "[['?', 0.08], [',', 0.22], ['class', 0.22], ['come', 0.22], ['do', 0.22], ['everyone', 0.22], ['have', 0.22], ['here', 0.22], ['now', 0.22], ['or', 0.22], ['should', 0.22], ['start', 0.22], ['to', 0.45], ['wait', 0.22], ['we', 0.45]]\n",
            "Similarity index with 3 documents in 0 shards (stored under newfolder)\n",
            "Number of documents: 3\n",
            "Comparing Result: [0.         0.01231762 0.8686709 ]\n",
            "0.8809885\n",
            "Average similarity float: 0.2936628262201945\n",
            "Average similarity percentage: 29.36628262201945\n",
            "Average similarity rounded percentage: 29\n",
            "Comparing Result: [0.59999996 0.         0.29898357]\n",
            "avg: 0.29966117938359577\n",
            "Comparing Result: [0.31622776 0.26469827 0.        ]\n",
            "avg: 0.1936420202255249\n",
            "Comparing Result: [0.         0.01231762 0.8686709 ]\n",
            "avg: 0.2936628262201945\n",
            "Similarity TF_A and TF_B: 0.02068965517241379\n",
            "Similarity idfs A and B: {'shop,': 0.6931471805599453, 'roof.': 0.6931471805599453, 'or': 0.0, 'have': 0.6931471805599453, 'is': 0.6931471805599453, 'wait': 0.0, 'find': 0.6931471805599453, 'tuna': 0.6931471805599453, 'can': 0.6931471805599453, 'of': 0.6931471805599453, 'should': 0.0, 'needunder': 0.6931471805599453, 'Should': 0.6931471805599453, 'love': 0.6931471805599453, 'here?': 0.0, 'for': 0.0, 'I': 0.6931471805599453, 'sandwiches?': 0.6931471805599453, 'great': 0.6931471805599453, 'bread': 0.6931471805599453, 'get': 0.6931471805599453, 'to': 0.0, 'kind': 0.6931471805599453, 'Malls': 0.0, 'eating': 0.6931471805599453, 'everyone': 0.0, 'one': 0.6931471805599453, 'What': 0.6931471805599453, 'toastedcheese': 0.6931471805599453, 'class': 0.0, 'and': 0.6931471805599453, 'everything': 0.6931471805599453, 'good': 0.6931471805599453, 'places': 0.6931471805599453, 'we': 0.0, 'used': 0.6931471805599453, 'start': 0.0, 'are': 0.0, 'Do': 0.6931471805599453, 'come': 0.6931471805599453, 'sandwiches.': 0.6931471805599453, 'now,': 0.0, 'shopping.': 0.6931471805599453}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}